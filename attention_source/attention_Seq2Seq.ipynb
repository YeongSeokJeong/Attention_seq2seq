{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "IlKnNyXlefih",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b74c4bdf-69cd-4846-cfcb-67ccdcd2cdd1"
      },
      "source": [
        "pip install --upgrade tensorflow"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/85/d4/c0cd1057b331bc38b65478302114194bd8e1b9c2bbc06e300935c0e93d90/tensorflow-2.1.0-cp36-cp36m-manylinux2010_x86_64.whl (421.8MB)\n",
            "\u001b[K     |████████████████████████████████| 421.8MB 36kB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.11.2)\n",
            "Requirement already satisfied, skipping upgrade: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied, skipping upgrade: gast==0.2.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.2.2)\n",
            "Requirement already satisfied, skipping upgrade: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.17.5)\n",
            "Collecting tensorflow-estimator<2.2.0,>=2.1.0rc0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/18/90/b77c328a1304437ab1310b463e533fa7689f4bfc41549593056d812fab8e/tensorflow_estimator-2.1.0-py2.py3-none-any.whl (448kB)\n",
            "\u001b[K     |████████████████████████████████| 450kB 42.8MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.12.0)\n",
            "Collecting tensorboard<2.2.0,>=2.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/40/23/53ffe290341cd0855d595b0a2e7485932f473798af173bbe3a584b99bb06/tensorboard-2.1.0-py3-none-any.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 50.8MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: scipy==1.4.1; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.10.0)\n",
            "Requirement already satisfied, skipping upgrade: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.1.8)\n",
            "Requirement already satisfied, skipping upgrade: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied, skipping upgrade: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied, skipping upgrade: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.33.6)\n",
            "Requirement already satisfied, skipping upgrade: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.9.0)\n",
            "Requirement already satisfied, skipping upgrade: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied, skipping upgrade: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.8.1)\n",
            "Requirement already satisfied, skipping upgrade: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.0.8)\n",
            "Requirement already satisfied, skipping upgrade: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow) (2.21.0)\n",
            "Requirement already satisfied, skipping upgrade: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow) (0.16.0)\n",
            "Collecting google-auth<2,>=1.6.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1c/6d/7aae38a9022f982cf8167775c7fc299f203417b698c27080ce09060bba07/google_auth-1.11.0-py2.py3-none-any.whl (76kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 11.8MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow) (0.4.1)\n",
            "Requirement already satisfied, skipping upgrade: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow) (42.0.2)\n",
            "Requirement already satisfied, skipping upgrade: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow) (3.1.1)\n",
            "Requirement already satisfied, skipping upgrade: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow) (2.8.0)\n",
            "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow) (2.8)\n",
            "Requirement already satisfied, skipping upgrade: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow) (2019.11.28)\n",
            "Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow) (0.2.7)\n",
            "Requirement already satisfied, skipping upgrade: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow) (4.0.0)\n",
            "Requirement already satisfied, skipping upgrade: rsa<4.1,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow) (4.0)\n",
            "Requirement already satisfied, skipping upgrade: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow) (1.3.0)\n",
            "Requirement already satisfied, skipping upgrade: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow) (0.4.8)\n",
            "Requirement already satisfied, skipping upgrade: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow) (3.1.0)\n",
            "\u001b[31mERROR: tensorboard 2.1.0 has requirement grpcio>=1.24.3, but you'll have grpcio 1.15.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement google-auth~=1.4.0, but you'll have google-auth 1.11.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: tensorflow-estimator, google-auth, tensorboard, tensorflow\n",
            "  Found existing installation: tensorflow-estimator 1.15.1\n",
            "    Uninstalling tensorflow-estimator-1.15.1:\n",
            "      Successfully uninstalled tensorflow-estimator-1.15.1\n",
            "  Found existing installation: google-auth 1.4.2\n",
            "    Uninstalling google-auth-1.4.2:\n",
            "      Successfully uninstalled google-auth-1.4.2\n",
            "  Found existing installation: tensorboard 1.15.0\n",
            "    Uninstalling tensorboard-1.15.0:\n",
            "      Successfully uninstalled tensorboard-1.15.0\n",
            "  Found existing installation: tensorflow 1.15.0\n",
            "    Uninstalling tensorflow-1.15.0:\n",
            "      Successfully uninstalled tensorflow-1.15.0\n",
            "Successfully installed google-auth-1.11.0 tensorboard-2.1.0 tensorflow-2.1.0 tensorflow-estimator-2.1.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CpwCNxBat7qw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 593
        },
        "outputId": "f5f00809-c2cd-4f1c-d1c8-bcfa698b9e23"
      },
      "source": [
        "pip install konlpy"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting konlpy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/85/0e/f385566fec837c0b83f216b2da65db9997b35dd675e107752005b7d392b1/konlpy-0.5.2-py2.py3-none-any.whl (19.4MB)\n",
            "\u001b[K     |████████████████████████████████| 19.4MB 164kB/s \n",
            "\u001b[?25hCollecting JPype1>=0.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/04/90/a94a55a58edfd67360fef85894bfb136a2c28b2cc7227d3a44dc508d5900/JPype1-0.7.1-cp36-cp36m-manylinux1_x86_64.whl (2.3MB)\n",
            "\u001b[K     |████████████████████████████████| 2.3MB 40.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.6/dist-packages (from konlpy) (4.2.6)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.6/dist-packages (from konlpy) (1.17.5)\n",
            "Collecting colorama\n",
            "  Downloading https://files.pythonhosted.org/packages/c9/dc/45cdef1b4d119eb96316b3117e6d5708a08029992b2fee2c143c7a0a5cc5/colorama-0.4.3-py2.py3-none-any.whl\n",
            "Collecting tweepy>=3.7.0\n",
            "  Downloading https://files.pythonhosted.org/packages/36/1b/2bd38043d22ade352fc3d3902cf30ce0e2f4bf285be3b304a2782a767aec/tweepy-3.8.0-py2.py3-none-any.whl\n",
            "Collecting beautifulsoup4==4.6.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/d4/10f46e5cfac773e22707237bfcd51bbffeaf0a576b0a847ec7ab15bd7ace/beautifulsoup4-4.6.0-py3-none-any.whl (86kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 13.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (1.12.0)\n",
            "Requirement already satisfied: PySocks>=1.5.7 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (1.7.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (1.3.0)\n",
            "Requirement already satisfied: requests>=2.11.1 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (2.21.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (3.1.0)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.11.1->tweepy>=3.7.0->konlpy) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.11.1->tweepy>=3.7.0->konlpy) (2019.11.28)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.11.1->tweepy>=3.7.0->konlpy) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.11.1->tweepy>=3.7.0->konlpy) (1.24.3)\n",
            "Installing collected packages: JPype1, colorama, tweepy, beautifulsoup4, konlpy\n",
            "  Found existing installation: tweepy 3.6.0\n",
            "    Uninstalling tweepy-3.6.0:\n",
            "      Successfully uninstalled tweepy-3.6.0\n",
            "  Found existing installation: beautifulsoup4 4.6.3\n",
            "    Uninstalling beautifulsoup4-4.6.3:\n",
            "      Successfully uninstalled beautifulsoup4-4.6.3\n",
            "Successfully installed JPype1-0.7.1 beautifulsoup4-4.6.0 colorama-0.4.3 konlpy-0.5.2 tweepy-3.8.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EAupbH0_4gRd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "ad83ce6c-f161-4dae-d324-0192ad346488"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "print(tf.__version__)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AoK_3pKPYy1R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import tensorflow as tf\n",
        "from konlpy.tag import Kkma\n",
        "import time\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras import Model\n",
        "\n",
        "BATCH_SIZE = 128\n",
        "embedding_dim = 300\n",
        "units = 128\n",
        "\n",
        "data = pd.read_csv(\"data_new7.csv\",encoding = 'cp949')\n",
        "\n",
        "input_data = data.iloc[:,0].to_list()\n",
        "output_data = data.iloc[:,1].to_list()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BeARVoAKY18J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_vocab,output_vocab = set(),set()\n",
        "input_max_len = 0\n",
        "output_max_len = 0\n",
        "kkma = Kkma()\n",
        "morphs_input_sentence = []\n",
        "morphs_output_sentence = []"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Tthsv5KY2fI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for input_sentence,output_sentence in zip(input_data,output_data):\n",
        "    input_sentence = kkma.morphs(input_sentence)\n",
        "    output_sentence = kkma.morphs(output_sentence)\n",
        "    \n",
        "    morphs_input_sentence.append(input_sentence)\n",
        "    morphs_output_sentence.append(output_sentence)\n",
        "    \n",
        "    input_vocab.update(input_sentence)\n",
        "    output_vocab.update(output_sentence)\n",
        "    \n",
        "    input_steplen = len(input_sentence)\n",
        "    output_steplen = len(output_sentence)\n",
        "    \n",
        "    input_max_len = input_max_len if input_max_len > input_steplen else input_steplen\n",
        "    output_max_len = output_max_len if output_max_len > output_steplen else output_steplen"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D7_YlUJKY3Km",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_vocab = ['<p>', '<start>', '<end>'] + list(input_vocab)\n",
        "output_vocab = ['<p>', '<start>', '<end>'] + list(output_vocab)\n",
        "\n",
        "input_vocab_size = len(input_vocab)\n",
        "output_vocab_size = len(output_vocab)\n",
        "\n",
        "dic_input_vocab = {word:index for index,word in enumerate(input_vocab)}\n",
        "dic_output_vocab = {word:index for index,word in enumerate(output_vocab)}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9o7muggnZFZi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_tokens = []\n",
        "output_tokens = []\n",
        "for i,(inp_sen, targ_sen) in enumerate(zip(morphs_input_sentence, morphs_output_sentence)):\n",
        "    s_input_token = [dic_input_vocab['<start>']] + [dic_input_vocab[word] for word in inp_sen] + [dic_output_vocab['<end>']]\n",
        "    s_output_token = [dic_output_vocab['<start>']] + [dic_output_vocab[word] for word in targ_sen] + [dic_output_vocab[\"<end>\"]]\n",
        "    \n",
        "    input_tokens.append(s_input_token)\n",
        "    output_tokens.append(s_output_token)\n",
        "\n",
        "input_tokens = pad_sequences(input_tokens, input_max_len, padding = 'post')\n",
        "output_tokens = pad_sequences(output_tokens, output_max_len, padding = 'post')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9XLj46RgKF2e",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "c659754f-dc5e-4c27-d54d-30bf97d42b9b"
      },
      "source": [
        "print(s_input_token)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1, 21199, 1094, 5822, 395, 13707, 12579, 19500, 4049, 18942, 18911, 8200, 10628, 18942, 10442, 15970, 15384, 19756, 9404, 10141, 14206, 10733, 11048, 2]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MyeEOWjTsDnt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess(sentence, vocab = dic_input_vocab, max_sen_len = input_max_len):\n",
        "    sen = sentence\n",
        "    sen = kkma.morphs(sen)\n",
        "    print(sen)\n",
        "    sen = [[dic_input_vocab['<start>']] + [dic_input_vocab[word] for word in sen] + [dic_input_vocab[\"<end>\"]]]\n",
        "    sen = pad_sequences(sen, max_sen_len,padding = 'post')\n",
        "    return sen"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SE8s2oggacwU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, gru_hidden, batch_sz):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.batch_sz = batch_sz\n",
        "        self.gru_hidden = gru_hidden\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "        self.gru = tf.keras.layers.GRU(self.gru_hidden,\n",
        "                                       return_sequences=True,\n",
        "                                       return_state=True,\n",
        "                                       recurrent_initializer='glorot_uniform')\n",
        "\n",
        "    def call(self, x, hidden):\n",
        "        x = self.embedding(x)\n",
        "        output, state = self.gru(x, initial_state = hidden)\n",
        "        return output, state\n",
        "\n",
        "    def initialize_hidden_state(self):\n",
        "        return tf.zeros((self.batch_sz, self.gru_hidden))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tNOK3GTzadw6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encoder = Encoder(len(input_vocab), embedding_dim = embedding_dim, gru_hidden = units, batch_sz = BATCH_SIZE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PuHBbQ9EahnJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BahdanauAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self,units):\n",
        "        super(BahdanauAttention,self).__init__()\n",
        "        self.W1 = tf.keras.layers.Dense(units)\n",
        "        self.W2 = tf.keras.layers.Dense(units)\n",
        "        self.V = tf.keras.layers.Dense(1)\n",
        "        \n",
        "    def call(self, query,values):\n",
        "        #query => encoder hidden\n",
        "        #values => decoder hidden\n",
        "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
        "        # hidden_with_time_axis의 shape은 (batch_size, 1, hidden_size)이다.\n",
        "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
        "        # score shape => (batch_size, seq_len, 1)\n",
        "        attention_weights = tf.nn.softmax(score, axis = 1)\n",
        "        context_vector = attention_weights*values\n",
        "        context_vector = tf.reduce_sum(context_vector, axis = 1)\n",
        "        \n",
        "        return context_vector, attention_weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I4KvYn2Uah9H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "    def __init__(self,vocab_size, embedding_dim, gru_hidden, batch_sz):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.batch_sz = batch_sz\n",
        "        self.gru_hidden = gru_hidden\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "        self.gru = tf.keras.layers.GRU(self.gru_hidden,\n",
        "                                      return_sequences=True,\n",
        "                                      return_state = True,\n",
        "                                      recurrent_initializer='glorot_uniform')\n",
        "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "        self.attention = BahdanauAttention(self.gru_hidden)\n",
        "        \n",
        "    def call(self, x, gru_hidden, enc_output):\n",
        "        context_vector,attention_weights = self.attention(gru_hidden, enc_output)\n",
        "        x = self.embedding(x)\n",
        "        x = tf.concat([tf.expand_dims(context_vector,1),x], axis = -1)\n",
        "        output,state = self.gru(x)\n",
        "        output = tf.reshape(output,(-1,output.shape[2]))\n",
        "        x = self.fc(output)\n",
        "        return x, state, attention_weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f5npzl_BajZg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "decoder = Decoder(len(output_vocab), embedding_dim, units, BATCH_SIZE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dUsMqE0xak6Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits = True, reduction = 'none')\n",
        "def loss_function(real,pred):\n",
        "    mask = tf.math.logical_not(tf.math.equal(real,0))\n",
        "    loss_ = loss_object(real,pred)\n",
        "    \n",
        "    mask = tf.cast(mask,dtype = loss_.dtype)\n",
        "    loss_ *= mask\n",
        "    return tf.reduce_mean(loss_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kc5_oyO2amAP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@tf.function\n",
        "def train_step(inp, targ, enc_hidden):\n",
        "    loss = 0\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
        "\n",
        "        dec_hidden = enc_hidden\n",
        "\n",
        "        dec_input = tf.expand_dims([dic_output_vocab['<start>']] * BATCH_SIZE, 1)\n",
        "\n",
        "        for t in range(1, targ.shape[1]):\n",
        "            predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
        "\n",
        "            loss += loss_function(targ[:, t], predictions)\n",
        "\n",
        "            dec_input = tf.expand_dims(targ[:, t], 1)\n",
        "\n",
        "    batch_loss = (loss / int(targ.shape[1]))\n",
        "\n",
        "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "\n",
        "    gradients = tape.gradient(loss, variables)\n",
        "\n",
        "    optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "    return batch_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m2R34D5oanPw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "steps_per_epoch = len(input_tokens)//BATCH_SIZE"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yi8mnkaraopI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "706c05da-ddcb-4415-8de9-bb0d5b198822"
      },
      "source": [
        "EPOCHS = 10\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    start = time.time()\n",
        "\n",
        "    enc_hidden = encoder.initialize_hidden_state()\n",
        "    total_loss = 0\n",
        "\n",
        "    for batch in range(len(input_tokens)//BATCH_SIZE):\n",
        "        batch_input = input_tokens[batch*BATCH_SIZE: (batch+1)*BATCH_SIZE]\n",
        "        batch_output = output_tokens[batch*BATCH_SIZE: (batch+1)*BATCH_SIZE]\n",
        "\n",
        "        batch_loss = train_step(batch_input, batch_output, enc_hidden)\n",
        "        total_loss += batch_loss\n",
        "\n",
        "        if batch % 50 == 0:\n",
        "            print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                                         batch,\n",
        "                                                         batch_loss.numpy()))\n",
        "    # saving (checkpoint) the model every 2 epochs\n",
        "    \n",
        "    print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                      total_loss / steps_per_epoch))\n",
        "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 Loss 2.1359\n",
            "Epoch 1 Batch 50 Loss 2.2156\n",
            "Epoch 1 Batch 100 Loss 2.2058\n",
            "Epoch 1 Batch 150 Loss 2.1381\n",
            "Epoch 1 Batch 200 Loss 1.0275\n",
            "Epoch 1 Batch 250 Loss 1.0582\n",
            "Epoch 1 Batch 300 Loss 1.4102\n",
            "Epoch 1 Batch 350 Loss 1.0876\n",
            "Epoch 1 Batch 400 Loss 1.0844\n",
            "Epoch 1 Batch 450 Loss 1.1939\n",
            "Epoch 1 Batch 500 Loss 1.1252\n",
            "Epoch 1 Batch 550 Loss 1.0619\n",
            "Epoch 1 Batch 600 Loss 1.1160\n",
            "Epoch 1 Batch 650 Loss 0.9627\n",
            "Epoch 1 Batch 700 Loss 0.9948\n",
            "Epoch 1 Batch 750 Loss 1.0179\n",
            "Epoch 1 Loss 1.3604\n",
            "Time taken for 1 epoch 307.67921137809753 sec\n",
            "\n",
            "Epoch 2 Batch 0 Loss 1.1873\n",
            "Epoch 2 Batch 50 Loss 1.9099\n",
            "Epoch 2 Batch 100 Loss 1.8324\n",
            "Epoch 2 Batch 150 Loss 1.7557\n",
            "Epoch 2 Batch 200 Loss 0.8695\n",
            "Epoch 2 Batch 250 Loss 0.8747\n",
            "Epoch 2 Batch 300 Loss 1.1992\n",
            "Epoch 2 Batch 350 Loss 0.9305\n",
            "Epoch 2 Batch 400 Loss 0.9268\n",
            "Epoch 2 Batch 450 Loss 1.0254\n",
            "Epoch 2 Batch 500 Loss 0.9724\n",
            "Epoch 2 Batch 550 Loss 0.9196\n",
            "Epoch 2 Batch 600 Loss 0.9734\n",
            "Epoch 2 Batch 650 Loss 0.8522\n",
            "Epoch 2 Batch 700 Loss 0.8846\n",
            "Epoch 2 Batch 750 Loss 0.9198\n",
            "Epoch 2 Loss 1.1490\n",
            "Time taken for 1 epoch 239.70501828193665 sec\n",
            "\n",
            "Epoch 3 Batch 0 Loss 1.0898\n",
            "Epoch 3 Batch 50 Loss 1.6767\n",
            "Epoch 3 Batch 100 Loss 1.6200\n",
            "Epoch 3 Batch 150 Loss 1.5605\n",
            "Epoch 3 Batch 200 Loss 0.8089\n",
            "Epoch 3 Batch 250 Loss 0.7967\n",
            "Epoch 3 Batch 300 Loss 1.1138\n",
            "Epoch 3 Batch 350 Loss 0.8663\n",
            "Epoch 3 Batch 400 Loss 0.8684\n",
            "Epoch 3 Batch 450 Loss 0.9703\n",
            "Epoch 3 Batch 500 Loss 0.9176\n",
            "Epoch 3 Batch 550 Loss 0.8729\n",
            "Epoch 3 Batch 600 Loss 0.9225\n",
            "Epoch 3 Batch 650 Loss 0.8134\n",
            "Epoch 3 Batch 700 Loss 0.8352\n",
            "Epoch 3 Batch 750 Loss 0.8761\n",
            "Epoch 3 Loss 1.0541\n",
            "Time taken for 1 epoch 236.81669664382935 sec\n",
            "\n",
            "Epoch 4 Batch 0 Loss 1.0379\n",
            "Epoch 4 Batch 50 Loss 1.5310\n",
            "Epoch 4 Batch 100 Loss 1.4724\n",
            "Epoch 4 Batch 150 Loss 1.4076\n",
            "Epoch 4 Batch 200 Loss 0.7696\n",
            "Epoch 4 Batch 250 Loss 0.7301\n",
            "Epoch 4 Batch 300 Loss 1.0151\n",
            "Epoch 4 Batch 350 Loss 0.8223\n",
            "Epoch 4 Batch 400 Loss 0.8147\n",
            "Epoch 4 Batch 450 Loss 0.9147\n",
            "Epoch 4 Batch 500 Loss 0.8559\n",
            "Epoch 4 Batch 550 Loss 0.8170\n",
            "Epoch 4 Batch 600 Loss 0.8514\n",
            "Epoch 4 Batch 650 Loss 0.7447\n",
            "Epoch 4 Batch 700 Loss 0.7446\n",
            "Epoch 4 Batch 750 Loss 0.7821\n",
            "Epoch 4 Loss 0.9706\n",
            "Time taken for 1 epoch 233.27758717536926 sec\n",
            "\n",
            "Epoch 5 Batch 0 Loss 0.9153\n",
            "Epoch 5 Batch 50 Loss 1.3250\n",
            "Epoch 5 Batch 100 Loss 1.2020\n",
            "Epoch 5 Batch 150 Loss 1.0748\n",
            "Epoch 5 Batch 200 Loss 0.5695\n",
            "Epoch 5 Batch 250 Loss 0.4771\n",
            "Epoch 5 Batch 300 Loss 0.8614\n",
            "Epoch 5 Batch 350 Loss 0.6728\n",
            "Epoch 5 Batch 400 Loss 0.6235\n",
            "Epoch 5 Batch 450 Loss 0.7121\n",
            "Epoch 5 Batch 500 Loss 0.6970\n",
            "Epoch 5 Batch 550 Loss 0.6525\n",
            "Epoch 5 Batch 600 Loss 0.6581\n",
            "Epoch 5 Batch 650 Loss 0.5782\n",
            "Epoch 5 Batch 700 Loss 0.5491\n",
            "Epoch 5 Batch 750 Loss 0.6089\n",
            "Epoch 5 Loss 0.7676\n",
            "Time taken for 1 epoch 232.80014896392822 sec\n",
            "\n",
            "Epoch 6 Batch 0 Loss 0.6934\n",
            "Epoch 6 Batch 50 Loss 0.9496\n",
            "Epoch 6 Batch 100 Loss 0.7677\n",
            "Epoch 6 Batch 150 Loss 0.5842\n",
            "Epoch 6 Batch 200 Loss 0.3782\n",
            "Epoch 6 Batch 250 Loss 0.2807\n",
            "Epoch 6 Batch 300 Loss 0.6826\n",
            "Epoch 6 Batch 350 Loss 0.5480\n",
            "Epoch 6 Batch 400 Loss 0.5051\n",
            "Epoch 6 Batch 450 Loss 0.5876\n",
            "Epoch 6 Batch 500 Loss 0.5203\n",
            "Epoch 6 Batch 550 Loss 0.5440\n",
            "Epoch 6 Batch 600 Loss 0.5507\n",
            "Epoch 6 Batch 650 Loss 0.4669\n",
            "Epoch 6 Batch 700 Loss 0.4384\n",
            "Epoch 6 Batch 750 Loss 0.5290\n",
            "Epoch 6 Loss 0.5621\n",
            "Time taken for 1 epoch 232.25039434432983 sec\n",
            "\n",
            "Epoch 7 Batch 0 Loss 0.5608\n",
            "Epoch 7 Batch 50 Loss 0.6672\n",
            "Epoch 7 Batch 100 Loss 0.4655\n",
            "Epoch 7 Batch 150 Loss 0.3290\n",
            "Epoch 7 Batch 200 Loss 0.2903\n",
            "Epoch 7 Batch 250 Loss 0.2002\n",
            "Epoch 7 Batch 300 Loss 0.5855\n",
            "Epoch 7 Batch 350 Loss 0.4856\n",
            "Epoch 7 Batch 400 Loss 0.4225\n",
            "Epoch 7 Batch 450 Loss 0.5286\n",
            "Epoch 7 Batch 500 Loss 0.4571\n",
            "Epoch 7 Batch 550 Loss 0.4975\n",
            "Epoch 7 Batch 600 Loss 0.4860\n",
            "Epoch 7 Batch 650 Loss 0.4190\n",
            "Epoch 7 Batch 700 Loss 0.3810\n",
            "Epoch 7 Batch 750 Loss 0.4730\n",
            "Epoch 7 Loss 0.4460\n",
            "Time taken for 1 epoch 232.11762809753418 sec\n",
            "\n",
            "Epoch 8 Batch 0 Loss 0.5015\n",
            "Epoch 8 Batch 50 Loss 0.4185\n",
            "Epoch 8 Batch 100 Loss 0.3049\n",
            "Epoch 8 Batch 150 Loss 0.2106\n",
            "Epoch 8 Batch 200 Loss 0.2505\n",
            "Epoch 8 Batch 250 Loss 0.1535\n",
            "Epoch 8 Batch 300 Loss 0.5325\n",
            "Epoch 8 Batch 350 Loss 0.4448\n",
            "Epoch 8 Batch 400 Loss 0.3828\n",
            "Epoch 8 Batch 450 Loss 0.4918\n",
            "Epoch 8 Batch 500 Loss 0.4053\n",
            "Epoch 8 Batch 550 Loss 0.4529\n",
            "Epoch 8 Batch 600 Loss 0.4322\n",
            "Epoch 8 Batch 650 Loss 0.3839\n",
            "Epoch 8 Batch 700 Loss 0.3383\n",
            "Epoch 8 Batch 750 Loss 0.4407\n",
            "Epoch 8 Loss 0.3714\n",
            "Time taken for 1 epoch 235.0863754749298 sec\n",
            "\n",
            "Epoch 9 Batch 0 Loss 0.4180\n",
            "Epoch 9 Batch 50 Loss 0.3333\n",
            "Epoch 9 Batch 100 Loss 0.2254\n",
            "Epoch 9 Batch 150 Loss 0.1433\n",
            "Epoch 9 Batch 200 Loss 0.2196\n",
            "Epoch 9 Batch 250 Loss 0.1248\n",
            "Epoch 9 Batch 300 Loss 0.4886\n",
            "Epoch 9 Batch 350 Loss 0.4195\n",
            "Epoch 9 Batch 400 Loss 0.3399\n",
            "Epoch 9 Batch 450 Loss 0.4598\n",
            "Epoch 9 Batch 500 Loss 0.3700\n",
            "Epoch 9 Batch 550 Loss 0.4325\n",
            "Epoch 9 Batch 600 Loss 0.4021\n",
            "Epoch 9 Batch 650 Loss 0.3636\n",
            "Epoch 9 Batch 700 Loss 0.2980\n",
            "Epoch 9 Batch 750 Loss 0.4109\n",
            "Epoch 9 Loss 0.3269\n",
            "Time taken for 1 epoch 233.78283667564392 sec\n",
            "\n",
            "Epoch 10 Batch 0 Loss 0.4086\n",
            "Epoch 10 Batch 50 Loss 0.2785\n",
            "Epoch 10 Batch 100 Loss 0.1875\n",
            "Epoch 10 Batch 150 Loss 0.1136\n",
            "Epoch 10 Batch 200 Loss 0.2071\n",
            "Epoch 10 Batch 250 Loss 0.1109\n",
            "Epoch 10 Batch 300 Loss 0.4735\n",
            "Epoch 10 Batch 350 Loss 0.3943\n",
            "Epoch 10 Batch 400 Loss 0.3114\n",
            "Epoch 10 Batch 450 Loss 0.4376\n",
            "Epoch 10 Batch 500 Loss 0.4718\n",
            "Epoch 10 Batch 550 Loss 0.5041\n",
            "Epoch 10 Batch 600 Loss 0.4331\n",
            "Epoch 10 Batch 650 Loss 0.3676\n",
            "Epoch 10 Batch 700 Loss 0.2941\n",
            "Epoch 10 Batch 750 Loss 0.4013\n",
            "Epoch 10 Loss 0.3213\n",
            "Time taken for 1 epoch 231.72036266326904 sec\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rQPvYxuOr3iD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(sentence):\n",
        "    inp = preprocess(sentence)\n",
        "\n",
        "    inputs = tf.convert_to_tensor(inp)\n",
        "\n",
        "    result = ''\n",
        "    \n",
        "    hidden = [tf.zeros((1, units))]\n",
        "    enc_out, enc_hidden = encoder(inputs, hidden)\n",
        "\n",
        "    dec_hidden = enc_hidden\n",
        "    dec_input = tf.expand_dims([dic_input_vocab['<start>']], 0)\n",
        "\n",
        "    for t in range(output_max_len):\n",
        "        predictions, dec_hidden, attention_weights = decoder(dec_input, \n",
        "                                                             dec_hidden,\n",
        "                                                             enc_out)\n",
        "        attention_weights = tf.reshape(attention_weights, (-1,))\n",
        "        \n",
        "\n",
        "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "\n",
        "        result += output_vocab[predicted_id] + ' '\n",
        "        \n",
        "        if output_vocab[predicted_id] == '<end>':\n",
        "            return result, sentence\n",
        "        dec_input = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "    return result, sentence"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I3-6__kGapyg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def translate(sentence):\n",
        "    result, sentence = evaluate(sentence)\n",
        "    print('Input: %s' % (sentence))\n",
        "    print('Predicted translation: {}'.format(result))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "08eEfLXj5q0n",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "4b2dcb4f-c2fb-4e24-822a-b5f4576d3513"
      },
      "source": [
        "translate('재미없는 기계학습')"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['재미없', '는', '기계', '학습']\n",
            "Input: 재미없는 기계학습\n",
            "Predicted translation: 재미없 는 기계 타 고 <end> \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EBUV8mum5xLC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_data[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hDhAkpto5-9C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}